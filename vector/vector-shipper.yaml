sources:
  app_logs:
    type: file
    include:
      - /logs/app.log
    ignore_older_secs: 86400  # Optional: Ignore files older than 1 day
    multiline:
      start_pattern: '^\['  # Optional: If logs span multiple lines (like JSON)
      mode: continue_through  # Use a valid mode value here
      condition_pattern: '^\['  # Optional: Condition pattern for multiline aggregation
      timeout_ms: 1000  # Optional: Timeout for multiline aggregation

transforms:
  parse_logs:
    type: remap
    inputs:
      - app_logs
    source: |
      # Parse the log line using the Grok pattern
      parsed, err = parse_grok(.message, "%{IP:client} - - \\[%{TIMESTAMP_ISO8601:timestamp}\\] \\\"%{WORD:method} %{PATH:path} HTTP/%{NUMBER:http_version}\\\" %{NUMBER:status} %{NUMBER:bytes} \\\"%{URI:referrer}\\\" \\\"%{DATA:user_agent}\\\"")
      
      # Handle errors (e.g., if the log line doesn't match the pattern)
      if err != null {
        log("Failed to parse log line: " + string!(err), level: "error")
      } else {
        # Merge the parsed fields into the event
        . = merge(., parsed)

        # Extract the "level" from the "path" field
        if .path == "/" {
          .level = "info"
        } else {
          .level = split(.path, "/")[1] ?? "unknown"
        }

        # Remove the "path" field
        del(.path)
      }

sinks:
  kafka:
    type: kafka
    inputs:
      - parse_logs
    bootstrap_servers: broker:29092
    topic: app_logs_topic
    encoding:
      codec: json